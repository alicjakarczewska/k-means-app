# Base libraries
import numpy as np
import scipy as sp

# Visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

import streamlit as st

# Algorithm testing libraries
from sklearn.cluster import KMeans

from sklearn.metrics.pairwise import manhattan_distances

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
# from sklearn.metrics import jaccard_similarity_score
from sklearn.metrics import jaccard_score
from scipy.spatial import distance
# distance.chebyshev([1, 0, 0], [0, 1, 0])
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import silhouette_score

st.set_page_config(
     page_title="K-means App",
    #  layout="wide",
    #  initial_sidebar_state="collapsed",
    )

url = 'IRISDAT.TXT'
df = pd.read_csv(url, sep=',', comment='#') 


with st.sidebar:    
    # st.subheader('Info')
    exp_1 = st.expander("Info", expanded=False)
    with exp_1:
        repo = '[Github](http://github.com)'
        ak = '[Alicja Karczewska](http://github.com)'
        mc = '[Marek Czarkowski](http://github.com)'
        st.subheader('Repozytorium kodu')
        st.markdown(repo, unsafe_allow_html=True)
        st.subheader('Autorzy')
        st.markdown(ak, unsafe_allow_html=True)
        st.markdown(mc, unsafe_allow_html=True)
    exp_2 = st.expander("Grupowanie", expanded=False)
    with exp_2:
        st.markdown('W celu rozwiÄ…zania problemu grupowania (analizy skupieÅ„) zastosowano metodÄ™ k-Å›rednich. \
            Jako dane wejÅ›ciowe pobiera ona zbiÃ³r danych z obiektami bez okreÅ›lonych klas decyzyjnych oraz \
            liczbÄ™ naturalnÄ… k okreÅ›lajÄ…cÄ… liczbÄ™ klas, na ktÃ³re wejÅ›ciowy zbiÃ³r danych ma zostaÄ‡ podzielony. \
            W wyniku dla kaÅ¼dego z obiektÃ³w powinna zostaÄ‡ przypisana \
            klasa decyzyjna (liczba z zakresu 1 ... k)\n \
            Metoda wyboru poczÄ…tkowego zestawu Å›rednich moÅ¼e byÄ‡ dowolna. \n\n ZostaÅ‚y uÅ¼yte \
            nastÄ™pujÄ…ce metryki oceny odlegÅ‚oÅ›ci obiektÃ³w: odlegÅ‚oÅ›Ä‡ euklidesowa, L1, \
            L-nieskoÅ„czonoÅ›Ä‡, Mahalanobisa.')
    exp_3 = st.expander("Metoda k-Å›rednich", expanded=False)
    with exp_3:
        st.markdown('Metoda k-Å›rednich jest metodÄ… naleÅ¼Ä…cÄ… do grupy algorytmÃ³w analizy skupieÅ„ tj. analizy \
            polegajÄ…cej na szukaniu i wyodrÄ™bnianiu grup obiektÃ³w podobnych (skupieÅ„) . Reprezentuje \
            ona grupÄ™ algorytmÃ³w niehierarchicznych. GÅ‚Ã³wnÄ… rÃ³Å¼nicÄ… pomiÄ™dzy niehierarchicznymi \
            i hierarchicznymi algorytmami jest koniecznoÅ›Ä‡ wczeÅ›niejszego podania iloÅ›ci skupieÅ„. \
            Przy pomocy metody k-Å›rednich zostanie utworzonych k rÃ³Å¼nych moÅ¼liwie odmiennych \
            skupieÅ„. Algorytm ten polega na przenoszeniu obiektÃ³w ze skupienia do skupienia tak dÅ‚ugo \
            aÅ¼ zostanÄ… zoptymalizowane zmiennoÅ›ci wewnÄ…trz skupieÅ„ oraz pomiÄ™dzy skupieniami. \
            Oczywistym jest, iÅ¼ podobieÅ„stwo w skupieniu powinno byÄ‡ jak najwiÄ™ksze, zaÅ› osobne \
            skupienia powinny siÄ™ maksymalnie od siebie rÃ³Å¼niÄ‡.')
    exp_4 = st.expander("Opis algorytmu", expanded=False)
    with exp_4:
        st.write('Zasada dziaÅ‚ania zaimplementowanego algorytmu jest nastÄ™pujÄ…ca: \
            \n\n1. Ustalamy liczbÄ™ skupieÅ„. \
            \n2. Ustalamy wstÄ™pne Å›rodki skupieÅ„. \
            Åšrodki skupieÅ„ tak zwane centroidy moÅ¼emy dobraÄ‡ na kilka sposobÃ³w: losowy \
            wybÃ³r k obserwacji, wybÃ³r k pierwszych obserwacji, dobÃ³r w taki sposÃ³b, aby \
            zmaksymalizowaÄ‡ odlegÅ‚oÅ›ci skupieÅ„. W przypadku implementacji naszego projektu \
            zdecydowaliÅ›my siÄ™ na wybÃ³r k pierwszych obserwacji. \
            \n3. Obliczamy odlegÅ‚oÅ›ci obiektÃ³w od Å›rodkÃ³w skupieÅ„. \
            WybÃ³r metryki jest bardzo istotnym etapem w algorytmie. WpÅ‚ywa ona na to, ktÃ³re z \
            obserwacji bÄ™dÄ… uwaÅ¼ane za podobne, a ktÃ³re za zbyt rÃ³Å¼niÄ…ce siÄ™ od siebie. \
            NajczÄ™Å›ciej stosowanÄ… odlegÅ‚oÅ›ciÄ… jest odlegÅ‚oÅ›Ä‡ euklidesowa. Nasza aplikacja \
            umoÅ¼liwia wybÃ³r nastÄ™pujÄ…cych metryk: euklidesowej, L1, L nieskoÅ„czonoÅ›Ä‡, \
            Mahalanobisa.\
            \n4. Przypisujemy obiekty do skupieÅ„\n\
            Dla danej obserwacji porÃ³wnujemy odlegÅ‚oÅ›ci od wszystkich skupieÅ„ i przypisujemy \
            jÄ… do skupienia, do ktÃ³rego Å›rodka ma najbliÅ¼ej.\
            \n5. Ustalamy nowe Å›rodki skupieÅ„\n\
            Nowym Å›rodkiem skupienia jest punkt, ktÃ³rego wspÃ³Å‚rzÄ™dne sÄ… Å›redniÄ… arytmetycznÄ… \
            wspÃ³Å‚rzÄ™dnych punktÃ³w naleÅ¼Ä…cych do danego skupienia.\
            \n6. Wykonujemy kroki 3,4,5 do czasu, aÅ¼ warunek zatrzymania zostanie speÅ‚niony.\
            \n7. NajczÄ™Å›ciej stosowanym warunkiem stopu jest iloÅ›Ä‡ iteracji zadana na poczÄ…tku lub \
            brak przesuniÄ™Ä‡ obiektÃ³w pomiÄ™dzy skupieniami.\
            \n\nFunkcja odpowiadajÄ…ca za przeprowadzenie algorytmu jako argumenty pobiera badany\
            zbiÃ³r danych, liczbÄ™ klastrÃ³w, metrykÄ™, liczbÄ™ iteracji. Pozwala na ukazanie porÃ³wnania\
            liczebnoÅ›ci klas wzglÄ™dem liczebnoÅ›ci utworzonych klastrÃ³w, utworzenie macierzy pomyÅ‚ek\
            (przypisanie odpowiednich nazw klas do poszczegÃ³lnych klastrÃ³w <klastry sÄ… oznaczone\
            kolejnymi liczbami naturalnymi> wymaga samodzielnego dopasowania przez uÅ¼ytkownika) i\
            dokÅ‚adnoÅ›ci (accuracy), a takÅ¼e wyliczenie miar podobieÅ„stwa (jaccard score, silhouette\
            score, cosine similarity) miÄ™dzy znalezionym zbiorem klastrÃ³w i zbiorem podzielonym\
            wzglÄ™dem klas z danych.\
            \n\nAplikacja pozwala takÅ¼e na wyÅ›wietlenie wykresu Å‚okciowego, ktÃ³ry pomaga w wyborze\
            optymalnej liczby klastrÃ³w dla badanego zbioru.')
    exp_5 = st.expander("Miary podobieÅ„stwa", expanded=False)
    with exp_5:
        st.write('Indeks Jaccarda, wspÃ³Å‚czynnik podobieÅ„stwa Jaccarda â€“ statystyka uÅ¼ywana do \
            porÃ³wnywania zbiorÃ³w. WspÃ³Å‚czynnik Jaccarda mierzy podobieÅ„stwo miÄ™dzy dwoma \
            zbiorami i jest zdefiniowany jako iloraz mocy czÄ™Å›ci wspÃ³lnej zbiorÃ³w i mocy sumy tych \
            zbiorÃ³w. WartoÅ›ci przyjmowane przez wspÃ³Å‚czynnik Jaccarda zawierajÄ… siÄ™ w podzbiorze \
            zbioru liczb rzeczywistych <0,1>. JeÅ›li wspÃ³Å‚czynnik Jaccarda przyjmuje wartoÅ›ci bliskie \
            zeru, zbiory sÄ… od siebie rÃ³Å¼ne, natomiast gdy jest bliski 1, zbiory sÄ… do siebie podobne. \
            \n\nSilhouette Coefficient (silhouette score) jest miarÄ… uÅ¼ywanÄ… do obliczenia jakoÅ›ci techniki \
            grupowania. Jego wartoÅ›Ä‡ waha siÄ™ od -1 do 1: \n * 1: Zgrupowania Å›rednich sÄ… dobrze od siebie oddzielone i wyraÅºnie rozrÃ³Å¼nione. \
            \n* 0: OdlegÅ‚oÅ›Ä‡ miÄ™dzy skupieniami nie jest znaczÄ…ca. \
            \n* -1: Oznacza, Å¼e klastry sÄ… przypisane w niewÅ‚aÅ›ciwy sposÃ³b \
            \n\n\nPodobieÅ„stwo cosinusowe to miara, ktÃ³ra okreÅ›la iloÅ›ciowo podobieÅ„stwo miÄ™dzy dwoma \
            lub wiÄ™cej wektorami. To cosinus kÄ…ta miÄ™dzy wektorami. Wektory sÄ… zwykle niezerowe i \
            znajdujÄ… siÄ™ w wewnÄ™trznej przestrzeni iloczynu. \
            Oblicza podobieÅ„stwo cosinusÃ³w miÄ™dzy prÃ³bkami w zbiorach X i Y, jako znormalizowany \
            iloczyn skalarny X i Y: K(X, Y) = <X, Y> / (||X||*||Y||) ')
    exp_6 = st.expander("ZbiÃ³r danych", expanded=False)
    with exp_6:
        st.write('Badania przeprowadzono na zbiorze Iris')
        df_csv = df.to_csv(index=False).encode('utf-8')
        st.download_button(label='ğŸ“¥ Pobierz zbiÃ³r danych',
                                        data=df_csv ,
                                        file_name= 'IRISDAT.csv')

all_res = []

column_list = df.columns

cols = ['LISDLG', 'LISSZE', 'PLADLG', 'PLASZE']
crim_lstat_array = np.array(df[cols])

def recalculate_clusters(X, centroids, k, dist):
    """ Recalculates the clusters """
    # Initiate empty clusters
    clusters = {}
    # Set the range for value of k (number of centroids)
    for i in range(k):
        clusters[i] = []
    # Setting the plot points using dataframe (X) and the vector norm (magnitude/length)
    for data in X:
        if(dist == "euc"):
            # Set up list of euclidian distance and iterate through
            euc_dist = []
            for j in range(k):
                euc_dist.append(np.linalg.norm(data - centroids[j]))
            # Append the cluster of data to the dictionary
            clusters[euc_dist.index(min(euc_dist))].append(data)
        if(dist == "l1"):
            # Set up list of l1 distance and iterate through
            l1_dist = []
            for j in range(k):
                l1_dist.append(manhattan_distances([data], [centroids[j]]))
            # Append the cluster of data to the dictionary
            clusters[l1_dist.index(min(l1_dist))].append(data)
        if(dist == "chebyshev"):
            # Set up list of chebyshev distance and iterate through
            chebyshev_dist = []
            for j in range(k):
                # chebyshev_dist.append(distance.chebyshev([data], [centroids[j]]))
                chebyshev_dist.append(np.max(np.abs(data - centroids[j])))
            # Append the cluster of data to the dictionary
            clusters[chebyshev_dist.index(min(chebyshev_dist))].append(data)
        if(dist == "mahal"):
            # Set up list of mahal distance and iterate through
            mahal_dist = []
            # iv = np.cov(X)
            iv = covMatrix
            for j in range(k):
                mahal_dist.append(distance.mahalanobis([data], [centroids[j]], iv))
            # Append the cluster of data to the dictionary
            clusters[mahal_dist.index(min(mahal_dist))].append(data)
    return clusters    
 
def recalculate_centroids(centroids, clusters, k):
    """ Recalculates the centroid position based on the plot """ 
    for i in range(k):
        # Finds the average of the cluster at given index
        centroids[i] = np.average(clusters[i], axis=0)
    return centroids

def plot_clusters(centroids, clusters, k):
    """ Plots the clusters with centroid and specified graph attributes """ 
    colors = ['red', 'blue' , 'green', 'orange', 'blue', 'gray', 'yellow', 'purple']
    fig = plt.figure(figsize = (6, 4))  
    area = (20) ** 2
    for i in range(k):
        st.write(f'Klaster nr {i % k} jest oznaczony kolorem: {colors[i % k]}')
        for cluster in clusters[i]:
            plt.scatter(cluster[0], cluster[1], c=colors[i % k], alpha=0.6)          
        plt.scatter(centroids[i][0], centroids[i][1], c='black', s=200)
    st.write(fig.figure)

def k_means_clustering(X, centroids={}, k=3, repeats=10, dist="euc"):
    """ Calculates full k_means_clustering algorithm """
    for i in range(k):
        # Sets up the centroids based on the data
        centroids[i] = X[i]
        # st.write(f"centroids: {centroids[i]}")

    # Outputs the recalculated clusters and centroids 
    st.subheader('Wykres danych przy pierwszej i ostatniej iteracji')
    for i in range(repeats):        
        clusters = recalculate_clusters(X, centroids, k, dist)  
        centroids = recalculate_centroids(centroids, clusters, k)

        # Plot the first and last iteration of k_means given the repeats specified
        # Default is 10, so this would output the 1st iteration and the 10th        
        if i == range(repeats)[-1] or i == range(repeats)[0]:
            plot_clusters(centroids, clusters, k)

    df1 = pd.DataFrame([ [key, len(value)] for key, value in clusters.items()], columns = ['Cluster', 'Number'])
    st.subheader(f"LicznoÅ›Ä‡ klastrÃ³w")
    st.write(df1)
    st.subheader("LicznoÅ›Ä‡ rzeczywistych klas danych")
    st.write(df['ODMIRYS'].value_counts()) 

    new_df = pd.DataFrame()

    for cluster in clusters:
        for index, value in enumerate(clusters[cluster]):
            df_with_cluster = pd.DataFrame.from_dict(clusters[cluster])
        df_with_cluster['cluster'] = cluster
        
        new_df = pd.concat([new_df, df_with_cluster], axis=0)
        new_df.reset_index(drop=True)
    
    df_to_join_1 = df.sort_values(by=cols).reset_index(drop=True)
    df_to_join_2 = new_df.sort_values(by=[0, 1, 2, 3]).drop_duplicates().reset_index(drop=True)
    df_info = pd.merge(df_to_join_1, df_to_join_2,  how='left', left_on=cols, right_on = [0, 1, 2, 3])
    
    df_info["cluster2"] = df_info["cluster"].map({0: 'SETOSA', 1:'VIRGINIC', 2:'VERSICOL'}) 
    df_info["ODMIRYS_to_num"] = df_info["ODMIRYS"].map({'SETOSA': 0, 'VIRGINIC': 1, 'VERSICOL':2}) 

    df_confusion = pd.crosstab(df_info["cluster2"], df_info["ODMIRYS"], rownames=['Actual'], colnames=['Predicted'], margins=True)

    st.subheader("Macierz pomyÅ‚ek - confusion matrix")
    st.dataframe(df_confusion)

    st.subheader("Miary podobieÅ„stwa")
    accuracy_sc = accuracy_score(df_info["cluster2"], df_info["ODMIRYS"])
    st.write(f'accuracy_score: {accuracy_sc}')

    jaccard_sc = jaccard_score(df_info["cluster2"], df_info["ODMIRYS"], average=None)
    st.write(f'jaccard_score: {jaccard_sc}')

    silhouette_sc = silhouette_score(df_info[['LISDLG', 'LISSZE', 'PLADLG', 'PLASZE',"ODMIRYS_to_num"]], df_info["cluster"])
    st.write(f'silhouette_sc: {silhouette_sc}')
    
    cosine_sc = cosine_similarity(np.asmatrix(df_info["cluster"]), np.asmatrix(df_info["ODMIRYS_to_num"]))[0][0]
    st.write(f'cosine_similarity: {cosine_sc}')

    
    res = pd.DataFrame([{'metrics':dist, 'accuracy_score':accuracy_sc, 'jaccard_score':jaccard_sc, 'silhouette_score':silhouette_sc, 'cosine_similarity':cosine_sc}])
    all_res.append(res)

st.title("GRUPOWANIE (ANALIZA SKUPIEÅƒ)")
st.header("1. Metryka euklidesowa")
k_means_clustering(crim_lstat_array, k=3, repeats=20, dist="euc")
plt.clf()

st.header("2. Metryka Manhattan - L1 - taksÃ³wkowa")
k_means_clustering(crim_lstat_array, k=3, repeats=20, dist="l1")
plt.clf()

st.header("3. Metryka Chebysheva - Lâˆ - maksimowa")
k_means_clustering(crim_lstat_array, k=3, repeats=20, dist="chebyshev")
plt.clf()

st.header("4. Metryka Mahalanobisa")
data = np.array([df['LISDLG'],df['LISSZE'],df['PLADLG'],df['PLASZE']])
covMatrix = np.cov(data,bias=True)
st.write(covMatrix)
k_means_clustering(crim_lstat_array, k=3, repeats=10, dist="mahal")
plt.clf()


def sklearn_k_means(X, k=3, iterations=10):
    """ KMeans from the sklearn algorithm for comparison to algo from scratch """ 
    km = KMeans(
        n_clusters=k, init='random',
        n_init=iterations, max_iter=300, 
        random_state=0
    )

    y_km = km.fit_predict(X)

    plt.clf()

    # plot the 3 clusters
    fig1 = plt.scatter(
        X[y_km == 0, 0], X[y_km == 0, 1],
        s=50, 
        c='blue',
        label='cluster 1'
    )

    plt.scatter(
        X[y_km == 1, 0], X[y_km == 1, 1],
        s=50, 
        c='red',
        label='cluster 2'
    )

    plt.scatter(
        X[y_km == 2, 0], X[y_km == 2, 1],
        s=50, 
        c='green',
        label='cluster 3'
    )

    # plot the centroids
    for i in range(3):
        plt.scatter(
            km.cluster_centers_[-i, 0], km.cluster_centers_[-i, 1],
            s=100,
            c='black',
            label='centroids'
        )
    plt.legend(scatterpoints=1)
    plt.title('Clustered Data')
    plt.grid()
    plt.show()
    st.write("fig1")
    st.write(fig1.figure)

    plt.clf()

    # "Elbow Plot" to demonstrate what is essentially the usefulness of number for k
    # Calculate distortion (noise) for a range of number of cluster
    distortions = []
    for i in range(1, 11):
        km = KMeans(
            n_clusters=i, init='random',
            n_init=10, max_iter=300,
            tol=1e-04, random_state=0
        )
        km.fit(X)
        distortions.append(km.inertia_)

    # Plot k vs distortion
    plt.plot(range(1, 11), distortions, marker='o')
    plt.xlabel('Number of clusters')
    fig2 = plt.ylabel('Distortion')
    plt.show()
    st.subheader("Wykres Å‚okciowy - Elbow plot")
    st.write(fig2.figure)

def elbow_plot(X, k=3, iterations=10):
    plt.clf()

    # "Elbow Plot" to demonstrate what is essentially the usefulness of number for k
    # Calculate distortion (noise) for a range of number of cluster
    distortions = []
    for i in range(1, 11):
        km = KMeans(
            n_clusters=i, init='random',
            n_init=10, max_iter=300,
            tol=1e-04, random_state=0
        )
        km.fit(X)
        distortions.append(km.inertia_)

    # Plot k vs distortion
    plt.plot(range(1, 11), distortions, marker='o')
    plt.xlabel('Number of clusters')
    fig2 = plt.ylabel('Distortion')
    plt.show()
    st.subheader("Wykres Å‚okciowy - Elbow plot")
    st.write(fig2.figure)


# sklearn_k_means(crim_lstat_array)
elbow_plot(crim_lstat_array)


df_res = pd.concat(all_res).reset_index(drop=True)
st.title("Podsumowanie")
st.header("Zestawienie wynikÃ³w miar podobieÅ„stwa dla poszczegÃ³lnych metryk")
st.write(df_res)
